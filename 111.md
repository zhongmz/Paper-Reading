


### ADePT

**Title** ADePT: Auto-encoder based Differentially Private Text Transformation》（EACL 2021 short paper）<br>

**Author:**Satyapriya Krishna;Rahul Gupta;Christophe Dupuy(Amazon Alexa)<br>

*注：这个模型的隐私强度分析存在错误（由When differential privacy meets NLP这篇文章提出）<br>*

**动机 ** 	在进行包含敏感信息的数据集的Differentially Private Text Transformation任务中中添加了噪声的这种转换算法的精度较差。因此作者通过使用自动编码器来提供一种保持精度的Differentially Private Text Transformation算法。<br>

**模型**     一个文本重写的auto-encoder:<br>

![image-20220320201643519](https://s2.loli.net/2022/03/20/Tb2OztdRQJhMy61.png)<br>

<br>

**主要思想**  1. 对encoder产生的中间向量进行裁剪加噪（这样加噪的好处是1.可以很容易地分析其隐私保护强度（指定Laplace参数--->推出差分隐私参数） 2. 中间向量长得更像比最终的输出更像会使得精度的损失少一些）3. Enc和Dec使用的都是简单的单向LSTM模型 4. 最后的使用有MIA（member Inference Attack分析，一种对训练集数据泄露程度量化分析）<br>

**实验**    <br>

 *数据集：ATIS SNIPS* <br>

*噪声选取：Gaussian and Laplacian noises<br>*

*噪声参数选取：*使用离散值 (1, 6, 9, 15, 28, 100)和(0.25, 0.5, 0.6, 0.75, 0.85, 1)<br>

*baseline：*[1910.08917.pdf (arxiv.org)](https://arxiv.org/pdf/1910.08917.pdf)<br>

*result*<br>

![image-20220320203425629](https://s2.loli.net/2022/03/20/72ahFR4CXpJqsoM.png)

![image-20220320203503246](https://s2.loli.net/2022/03/20/3MPWqLnlBuQ2Sgs.png)

<br>

by *Meizhi Zhong*

<br>

### When differential privacy meets NLP

****

**Title：**When differential privacy meets NLP: The devil is in the detail<br>

**Author:** Ivan Habernal<br>

**动机：  **理论推导ADePT的分析错误，也记录一下关于differential privacy 的一些背景知识和推理<br>

​		从高层次的角度来看，DP使用的是个人的概念，其信息保存在数据库（数据集）中。每个人的数据点（或记录）可以是一个位、一个数字、一个向量、一个结构化记录、一个文本文档或任何任意对象，都被认为是私有的，不能公开。此外，即使数据库中是否有任何特定的个人，也被认为是私有的。<br>

**Definition 2.1** **隐私保护对象的定义**<br>

![image-20220320204003622](https://s2.loli.net/2022/03/20/CzG9SPMyUAsXrcl.png)

**Definition 2.2** **随机算法的定义<br>**

<img src="https://s2.loli.net/2022/03/20/ZkO6bK39tP8aHNy.png" alt="image-20220320204014585" style="zoom:50%;" /><br>

**Definition 2.3** **DP隐私保护机制的定义：输出的越像，保护得越强<br>**

 ![image-20220320204031569](https://s2.loli.net/2022/03/20/LP7Ss5z8GnF1yrM.png)

​		可以从上式得到（或者理解为Differential Privacy这件事情要做的就是让原本的样本x和y更相似<br>

![image-20220320204125194](https://s2.loli.net/2022/03/20/pb3mQNHDcjagsnU.png)<br>

**Definition 2.4 ** **L1敏感度定义**<br>

![image-20220320204236164](https://s2.loli.net/2022/03/20/OU1i47aHRLVyBZe.png)<br>

**Definition 2.5**（Laplace概率密度定义）<br>

<img src="https://s2.loli.net/2022/03/20/RoDykxdZLe9qIaN.png" alt="image-20220320204318683" style="zoom: 50%;" /><br>

**Definition 2.6**（在原始映射中添加Laplace噪声，也就是开始**加噪**）<br>

<img src="https://s2.loli.net/2022/03/20/BMclixegDRkFaA4.png" alt="image-20220320204447841" style="zoom:50%;" /><br>

**证明**接下来是证明使用Laplace加噪后的函数满足<img src="https://s2.loli.net/2022/03/20/MNxW5gKG2317nuJ.png" alt="image-20220320204547143" style="zoom:50%;" /><br>

<img src="https://s2.loli.net/2022/03/20/BDUah8YrSo24jEH.png" alt="image-20220320204614597" style="zoom:50%;" /><br>

回顾 ADePT中的裁剪函数（对应Definition2.6）<br>

<img src="https://s2.loli.net/2022/03/20/T7efg38Yh9aCLUi.png" alt="image-20220320205143822" style="zoom:50%;" /><br>

<img src="https://s2.loli.net/2022/03/20/rbKAENTG3dyuw6x.png" alt="image-20220320205233868" style="zoom:50%;" /><br>

假设:<br>

<img src="https://s2.loli.net/2022/03/20/LG5a2yrv3Ns4Ihl.png" alt="image-20220320205424088" style="zoom: 50%;" /><br>

<img src="https://s2.loli.net/2022/03/20/rAoIbtWgVf1vH3D.png" alt="image-20220320205432424" style="zoom:50%;" /><br>

其中问题在于：		$\Delta f$ 不是2C，而是$2C\sqrt{n}$，并不满足$(\varepsilon,0)$-DP<br>

补救方法：<br>

1. 潜在矢量剪裁可以使用“1-范数”<br>
2. 拉普拉斯噪声可以使用确定的正确灵敏度$2C\sqrt{n}$<br>

但是因为两种方法都会导致噪声增加，使得任务的精度变差。<br>

by *Meizhi Zhong*<br>



### LDP-DL

**Title：**Locally Differentially Private Distributed Deep Learning via Knowledge Distillation<br>

**Author:** Di Zhuang, Mingchen Li and J. Morris Chang<br>

**动机：  ** 数据使用者（训练模型的人） 希望使用跨多个不同数据所有者分离的数据构建DL模型。然而，由于数据的敏感性质，这可能会导致严重的隐私问题，因此数据所有者会犹豫不决，不愿参与。<br>

**亮点：  **（和SeqPATE类似，换了一种故事）<br>

1. 一个新颖、有效和高效的**隐私保护分布式深度学习框架**，使用局部差异隐私和知识提取<br>
2. 一种主动抽样方法【**ActiveQuerySampling**】，减少从Data User到每个Data Owner的查询总数，从而降低隐私预算的总成本。（就是学生模型置信度低的预测对教师进行Query）<br>

**模型：   **作者提出了LDP-DL（Locally Differentially Private Distributed Deep Learning），这是一个通过**局部差异隐私**和**知识提炼**来保护隐私的分布式深度学习框架，其中**每个Data Owner使用自己的（局部）私有数据集学习一个教师模型，Data User学习一个学生模型来模拟教师模型集合的输出**。<br>

<img src="https://s2.loli.net/2022/03/20/aihSKwsRoOXE823.png" alt="image-20220318104020490" style="zoom:50%;" />

**蒸馏框架**<br>

<img src="https://s2.loli.net/2022/03/20/4LJpdiSFgaEIAvf.png" alt="image-20220318135125788" style="zoom:50%;" /><br>

**知识蒸馏的损失函数**<br>

- 使用多个温度<br>

![image-20220320210545107](https://s2.loli.net/2022/03/20/zLWMsxGFH7hcUV4.png)<br>

**实验**<br>

*数据集： CIFAR-10; MNIST; Fashion-MNIST* <br>

*baseline: DP-SGD;PATE;DP-FL*<br>



### RGP

**Title：**Large Scale Private Learning via Low-rank Reparametrization<br>

**Author:** Da Yu ; Huishuai Zhang ; Wei Chen;  Jian Yin;  Tie-Yan Liu<br>

**亮点**<br>

1. 第一个能够在BERT模型上应用**差异隐私**<br>
2. 在四个下游任务GLUE(MNLI, QQP, QNLI, SST-2)中，$\epsilon=8$的平均准确率为83.9%<br>
3. 提出了重新参数化梯度扰动（RGP），当在**大型模型上应用DP**时，可以降低内存成本并提高实用性。<br>



**动机**<br>

​		提出一种**重新参数化**方案（类似高效率参数），以应对在**大型神经网络**上应用**DP-SGD**所带来的问题：1）**存储**每个梯度的巨大**内存**成本，2）增加的噪声随着维数增加而增加。<br>

**模型**<br>

【改变权重矩阵更新的方式，并加噪声】<br>

​		reparametrized 保持了【forward/backward】过程不变，使得可以在**不计算梯度**的情况下计算投影梯度。为了使用差分隐私进行学习，作者设计了**重新参数化梯度扰动**【reparametrized gradient perturbation】（RGP），扰动梯度载波矩阵上的梯度，并根据**噪声梯度**重建原始权重的更新。<br>

![image-20220320212546464](https://s2.loli.net/2022/03/20/BXfnZlbxwqDKM5C.png)<br>

权重矩阵更新的细节如下：<br>

在每次更新时，对于具有权重矩阵W的层，RGP包括四个步骤：<br>

- 1）生成梯度载波矩阵L和R<br>
- 2）运行重新参数化的forward/backward过程并获得各个梯度$\{∂_iL\}^m_{ i=1}$和$\{∂_iR\}^m_{ i=1}$<br>
- 3）剪裁并扰动梯度（和梯度下降非常像）<br>
- 4）在原始权重矩阵上重建近似的梯度<br>

**Step1** ：**选择“好”“梯度载波矩阵**，使重建的梯度尽可能接近原始梯度。<br>

​		首先，这要求给定秩r，生成的梯度载波矩阵应与原始梯度的**主分量对齐**。此外，要在步骤4）中重建梯度，需要梯度载体具有**正交的列/行**。<br>

​		因此作者提出：<br>

1. 使用**历史更新来查找梯度载波**。由于DP的后处理特性，历史更新不敏感。<br>
2. 应用Gram-Schmidt过程对L和R进行正交归一化<br>

<img src="https://s2.loli.net/2022/03/20/6RC4hATFBkGoZn8.png" alt="image-20220308203642877" style="zoom:50%;" /><br>

**Step 2**<br>

​		重新参数化和一轮forward/backward传播<br>

**Step 3**<br>

​		提供隐私保护的步骤<br>

​		每个梯度$\{∂_iL \ ∂_iR\}^m_{ i=1}$首先通过预定义的阈值进行**剪裁**。然后，将高斯**噪声添加**到聚合梯度中，以建立差分隐私边界。添加的噪声的能量与维数成正比（载波矩阵的秩r）。因此，为了使噪声能量较小，应该使用较小的秩r。然而，较小的秩会增加步骤1）中的近似误差。（tradeoff的体现）<br>

**Step 4**<br>

​	使用加噪后的梯度来重建原始权重的梯度。然后，任何现成的优化器都可以使用重建的梯度<br>

**实验**<br>

*数据集：MNLI, QQP, QNLI, and SST-2 from GLUE*<br>

![image-20220308143503936](https://s2.loli.net/2022/03/20/Ul7AOzGXZMDNYVk.png)<br>

- 为什么RGP-random比RGP差？<br>

因为随机子空间不能像历史更新的子空间那样有效地捕捉梯度信息。（如何寻找梯度矩阵那一节）<br>

- 为什么单纯的DP-SGD【2016】最差？<br>

因为噪声压倒了梯度中的有用信息<br>

![image-20220308143353657](https://s2.loli.net/2022/03/20/uEfplzxXwSOW3UA.png)<br>










