# 前言

![image-20211207144743068](https://s2.loli.net/2021/12/07/RNQyvKFHZensmzk.png)



# 评估指标

## ROC

- Receiver Operating Characteristic

![image-20211106211726098](https://s2.loli.net/2021/12/07/q4QgXvMuxZS3Bhj.png)

- 点(0 ，1 ) 则对应于将所有正例排在所有反例之前的"理想模型"
- 若一个学习器的 ROC 曲线被另 一个学习器的曲线完全"包住"， 则可断言后者的性能优于前者
- 若两个学习 器的 ROC 曲线发生交叉，则难以一般性地断言两者孰优孰劣，可以使用AUC（比较 ROC 曲线下 的面积）





- iteration：进行训练需要的总共的迭代次数

- batchsize：进行一次iteration（迭代）所训练数据的数量

- epoch：一次epoch是指将所有数据训练一遍的次数，epoch所代表的数字是指所有数据被训练的总轮数

  例如：有49000个数据，计划进行十轮训练，那么epoch=10；一次训练迭代训练100个数据，batchsize=100，训练一轮总共要迭代490次（49000/100=490）。总的iteration=490*10=4900次





# CODE-基于N-gram的最大熵模型

1. 标注数据（用空格间隔）
2. 对每一行数据提取特征（计算每一种词的one-hot编码，转换为特征向量）
3. 最大熵模型计算分类

[最大熵分类器](https://github.com/Dod-o/Statistical-Learning-Method_Code/blob/master/Logistic_and_maximum_entropy_models/maxEntropy.py)





# PAPER-Neural Machine with Monolingual 

- Improving Neural Machine Translation Models with Monolingual Data
- 使用海量的单语语料去提升翻译模型的效果

- [机器翻译论文阅读-Back Translation - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/330866548)

- Monolingual&&Parallel：[语料库种类](https://zhuanlan.zhihu.com/p/59514775)



## Seq2Seq模型

- Our method uses a **multilayered Long Short-Term Memory (LSTM) **to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector.
- 基础知识：LSTM
- encoder && decoder
- [BLEU：一种自动评估机器翻译的方法 ](https://zhuanlan.zhihu.com/p/39100621)
- [【NLP-10】seq2seq - 忆凡人生 - 博客园 (cnblogs.com)](https://www.cnblogs.com/yifanrensheng/p/13167724.html)
- [Seq2Seq模型介绍 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/194308943)





















# CODE-CNN

[LeNet-5详解](https://cuijiahua.com/blog/2018/01/dl_3.html)

[LeNet-5 卷积神经网络结构](https://blog.csdn.net/strint/article/details/44163869)

- 连接方式

- 基于AlexNet提取的经典模型有：ZFNet，VGGNet，GoogleNet，ResNet

![image-20211122202349798](https://s2.loli.net/2021/12/07/j1GbXp5DUQKrFaA.png)

## 总体结构

- 输入层【数据处理部分】
- 卷积层
- 池化层：平均池化；最大池化
  - 降维
  - 挑选最具代表性的值
  - 节省计算量
  - 位置不变性



- 基于谷歌新闻的 预训练word2vec词向量
- aclIMDB - by Learing Word Vectors for Sentiment Analysis

![image-20211122201745901](https://s2.loli.net/2021/12/07/xv9GyKRfJqFgeoT.png)



![image-20211122155031495](https://s2.loli.net/2021/12/07/O8Tpe9JbcFKtDAS.png)

- convolutional layers are labeled Cx, subsampling layers are labeled Sx, and fully-connected layers are labeled Fx, where x is the layer index.

## C1

- 卷积层

- Layer C1 is a convolutional layer with 6 feature maps. Each unit in each feature map is connected to a 5x5 neighborhood in the input. 【C1有6个特征图，由6个5*5的卷积核映射得到】
- The size of the feature maps is 28x28 which prevents connection from the input from falling othe boundary. C1 contains 156 trainable parameters, and 122,304 connections.【156 = (5x5+1)x6，卷积核为5*5加上偏差，122304 =156x28x28】



## S2

- 下采样层

- Layer S2 is a sub-sampling layer with 6 feature maps of size 14x14. 
- Each unit in each feature map is connected to a 2x2 neighborhood in the corresponding feature map in C1. The four inputs to a unit in S2 are added, then multiplied by a trainable coefficient（系数）, and added to a trainable bias. 
- The result is passed through a sigmoidal function. The 2x2 receptive fields are non-overlapping, therefore feature maps in S2 have half the number of rows and column as feature maps in C1. 
- Layer S2 has 12 trainable parameters and 5,880 connections.【每一个下采样核有2个参数12=2x6；5880 = (4+1)x14x14x6】



## C2

- Layer C3 is a convolutional layer with 16 feature maps. Each unit in each feature map is connected to several 5x5 neighborhoods at identical locations in a subset of S2's feature maps. Table I shows the set of S2 feature maps

![image-20211123213403096](https://s2.loli.net/2021/12/07/lCKmIfZrA6wHTx4.png)

![](https://s2.loli.net/2021/12/07/FXmaHAhiRxev1kt.png)



- a break of symmetry (打破对称性) 

- Different feature maps are forced to extract different features (有利于提取多种组合特征)
- The first six C3 feature maps take inputs from every contiguous subsets of three feature maps in S2(前6个feature map（对应上图第一个红框的6列）与S2层相连的3个feature map相连接（上图第一个红框）)
- The next six take input from every contiguous subset of four.（后面6个feature map与S2层相连的4个feature map相连接（上图第二个红框））
- The next three take input from some discontinuous subsets of four（后面3个feature map与S2层部分不相连的4个feature map相连接，最后一个与S2层的所有feature map相连）
- Finally the last one takes input from all S2 feature maps（最后一个与S2层的所有feature map相连）
- 6x(3x5x5+1)+6x(4x5x5+1)+3x(4x5x5+1)+1x(6x5x5+1)=1516参数

## S4

- 16x（2x2+1）x5x5=2000

## C5

- 120x（16x5x5+1）=48120





# CODE-RNN

![image-20211123201402222](https://s2.loli.net/2021/12/07/aQutwRghbBZrmYn.png)

![RNN-unrolled](https://s2.loli.net/2021/12/07/EUNjPhxAO7MR4n8.png)



- 上一层的输出作为下一层的输入
- 计算各个时刻的权重校正值但是不会立即更新





# CODE-LSTM

- [[1909.09586\] Understanding LSTM -- a tutorial into Long Short-Term Memory Recurrent Neural Networks (arxiv.org)](https://arxiv.org/abs/1909.09586)
- [RNN 与 LSTM 的原理详解](https://blog.csdn.net/HappyRocking/article/details/83657993)
- 相比普通的RNN，LSTM能够在更长的序列中有更好的表现

## 基本结构

![image-20211124182852487](https://i.loli.net/2021/12/02/LnefvSpQHkPd1XF.png)



- 输入：（t-1时刻的输出，t时刻的输入）【拼接，比如输出向量维度是50，输入的向量表示维度是300，则50+300=350】；记忆向量
- 输出：t时刻的输出；新的记忆向量

### 遗忘门

![2102682-a4a419e455fdeb26](https://i.loli.net/2021/12/02/OJQRrSPTfKehlos.png)

- 只更新流入的**记忆向量**
- LSTM网络经过学习决定让网络记住多少以前的内容
- 【更新后的记忆向量 】=  【更新前记忆向量】 逐个元素乘  【$f_t$ 】

### 输入门

![2102682-2a745277a43ed1ab](https://i.loli.net/2021/12/02/FmDtQ3LUdao8cOi.png)

- 只更新流入的**记忆向量**
- sigmoid 决定什么值 要更新（或者什么值不要）
- tanh  创建一个新的候选值向量



![LSTM3-focus-C](https://i.loli.net/2021/12/02/6sTJEPOmeYch3HM.png)

- 记忆向量的更新到此结束
- 通过$f _t$选择哪些信息（记忆向量中）需要被遗忘，$i_t$ 选择哪些信息（输入的向量表示中）需要被记住
- 从而更新记忆向量



### 输出门

![LSTM3-focus-o](https://i.loli.net/2021/12/02/nDKLcX4AURohMGH.png)

- 唯一更新输出向量$h_t$的门
- 由$o_t$选择哪些**新的**记忆向量需要输出





# PAPER-seq2seq

- Sequence to Sequence Learning with Neural Networks

![img](https://pdf.cdn.readpaper.com/parsed/fetch_target/4c74116a86bf750934ab2fb5154a2cf7_1_Figure_1.png)







# PAPER-Transformer

- [Attention in RNN](https://zhuanlan.zhihu.com/p/42724582)

## 基本结构

- encoder-decoder结构

![img](https://pdf.cdn.readpaper.com/parsed/fetch_target/af91eb0d9382ed649e65f3d80bae456a_2_Figure_1.png)

- 整体结构

![img](http://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png)



## encoder块

### encoder块的结构

<img src="http://jalammar.github.io/images/t/encoder_with_tensors.png" alt="img" style="zoom:80%;" />



### 多头注意力层-Multi-Head Attention

- 输入：(Q,K,V)，一开始都是（词向量+顺序编码），后面是上一个encoder的输出
- 

![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BAttention%7D%28Q%2CK%2CV%29%3D%5Ctext%7Bsoftmax%7D%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%29V+%5Ctag1)

- 不妨令$Z =  相似度(Q,K)= Attention(Q,K,V)$

- 多头注意力层的内部结构

<img src="https://pdf.cdn.readpaper.com/parsed/fetch_target/af91eb0d9382ed649e65f3d80bae456a_3_Figure_2.png" alt="img" style="zoom: 67%;" />

- **左边：单个注意力层的结构**
  - MatMul：进行一次矩阵乘法 Q*K
  - Scale:       $\frac{Q*K} { \sqrt{d_k}}$，防止值向两侧聚拢，防止梯度过小
  - Mask:     在decoder中有用，t时刻后的值更换成SUP,softmax后得到对应位置的值为0
  - SoftMax: 激活函数
  - MatMul:  第2次矩阵乘法
- **右边：多头注意力**
  - ![image-20211201214336785](https://i.loli.net/2021/12/01/fuSBq4pabo8e2Zr.png)
  - 输入：(Q,K,V)
  - 学习：$W^Q ,W^K,W^V,W^O$,其中$W^Q ,W^K,W^V$有h个
  - 输出：Attention(Q,K,V)



#### 为什么使用多头注意力？

- Multi-head attention allows the model to jointly attend to information from **different representation subspaces** at different positions.
- 有助于网络捕捉到更丰富（不同子空间）的特征【多模态？】
- 类似CNN的filter?









### Feed Forward Neural Network

![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BFFN%7D%28Z%29+%3D+max%280%2C+ZW_1+%2Bb_1%29W_2+%2B+b_2+%5Ctag2)















## decoder块

### Masked Multi-Head Attention-1

- 输入：(Q,K,V)，是上一层decoder的输出
- 其他同 encoder

### Multi-Head Attention-2

- 输入：Q = 内部输出，(K,V) encoder的输出
- 其他同encoder

## batchNorm&&LayerNorm

![img](https://pic3.zhimg.com/80/v2-07b1c7f674673bb7eaea8004155be766_1440w.jpg?source=1940ef5c)

- shape记为[N, C, H, W]，N=BatchSize, C=channels, H=高， W=宽
- Batch Normalization 的处理对象是对一批样本，是对这批样本的同一维度特征做归一化，Batch Normalization，它去除了不同特征之间的大小关系，但是保留了不同样本间的大小关系，所以在CV领域用的多。
- Layer Normalization 的处理对象是单个样本，是对这单个样本的所有维度特征做归一化。即在通道方向上，对[C, H, W] 归一化，主要对RNN作用明显。Layer Normalization，它去除了不同样本间的大小关系，但是保留了一个样本内不同特征之间的大小关系，所以在NLP领域用的多。

![image-20211201131739038](https://s2.loli.net/2021/12/07/OUzSZRQE9hPH4aF.png)



## Positional Encoding【加入时序信息】

- RNN中添加时序信息：上一层的输出作为下一层的输入
- Transformer中添加时序信息：位置编码

 



