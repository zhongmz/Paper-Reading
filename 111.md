# daily-paper-reading

Short and daily paper reading

2022-03

<table style="width:100%">
  <tr>
    <th>Name</th>
    <th>Number</th>
    <th>Total</th>
  </tr>
  <tr>
    <td>Liang Ding</td>
    <td>0</td>
    <td rowspan=20>17</td>
  </tr>
  <tr>
    <td>*Keqin Peng</td>
    <td>0</td>
  </tr>
  <tr>
    <td>*Zheng Zhang</td>
    <td>0</td>
  </tr>
  <tr>
    <td>*Changtong Zan</td>
    <td>0</td>
  </tr>
  <tr>
    <td>*Qingyu Lu</td>
    <td>0</td>
  </tr>
  <tr>
    <td>*Qihuang Zhong</td>
    <td>0</td>
  </tr>
  <tr>
    <td>*Hexuan Deng</td>
    <td>0</td>
  </tr>
  <tr>
    <td>*Jun Rao</td>
    <td>0</td>
  </tr>
  <tr>
    <td>*Meizhi Zhong</td>
    <td>0</td>
  </tr>
  <tr>
    <td>*Fei Wang</td>
    <td>0</td>
  </tr>
  <tr>
    <td>*Chia-Ming Hsu</td>
    <td>0</td>
  </tr>
  <tr>
    <td>*Tengfei Yu</td>
    <td>0</td>
  </tr>
  <tr>
    <td>Yangjun Zhang</td>
    <td>0</td>
  </tr>
  <tr>
    <td>Ziheng Cheng</td>
    <td>0</td>
  </tr>
  <tr>
    <td>Fuqiang Yu</td>
    <td>0</td>
  </tr>
  <tr>
    <td>*Bing Wang</td>
    <td>0</td>
  </tr>
  <tr>
    <td>*Baopu Qiu</td>
    <td>17</td>
  </tr>
  <tr>
    <td>Runze Fan</td>
    <td>0</td>
  </tr>
  <tr>
    <td>Boan Liu</td>
    <td>0</td>
  </tr>
  <tr>
    <td>Shuai He</td>
    <td>0</td>
  </tr>
</table>



("*" means on-site members)

---

### ADePT

**Title：**ADePT: Auto-encoder based Differentially Private Text Transformation》（EACL 2021 short paper）<br>

**Author: **Satyapriya Krishna;Rahul Gupta;Christophe Dupuy(Amazon Alexa)<br>

*注：这个模型的隐私强度分析存在错误（由When differential privacy meets NLP这篇文章提出）<br>*

**动机 ** 	在进行包含敏感信息的数据集的Differentially Private Text Transformation任务中中添加了噪声的这种转换算法的精度较差。因此作者通过使用自动编码器来提供一种保持精度的Differentially Private Text Transformation算法。<br>

**模型**     一个文本重写的auto-encoder:<br>

![image-20220320201643519](C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320201643519.png)<br>

<br>

**主要思想**  1. 对encoder产生的中间向量进行裁剪加噪（这样加噪的好处是1.可以很容易地分析其隐私保护强度（指定Laplace参数--->推出差分隐私参数） 2. 中间向量长得更像比最终的输出更像会使得精度的损失少一些）3. Enc和Dec使用的都是简单的单向LSTM模型 4. 最后的使用有MIA（member Inference Attack分析，一种对训练集数据泄露程度量化分析）<br>

**实验**    <br>

 *数据集：ATIS SNIPS* <br>

*噪声选取：Gaussian and Laplacian noises<br>*

*噪声参数选取：*使用离散值 (1, 6, 9, 15, 28, 100)和(0.25, 0.5, 0.6, 0.75, 0.85, 1)<br>

*baseline：*[1910.08917.pdf (arxiv.org)](https://arxiv.org/pdf/1910.08917.pdf)<br>

*result*<br>

![image-20220320203425629](C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320203425629.png)

![image-20220320203503246](C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320203503246.png)

<br>

by *Meizhi Zhong*

<br>

### When differential privacy meets NLP

****

**Title：**When differential privacy meets NLP: The devil is in the detail<br>

**Author:** Ivan Habernal<br>

**动机：  **理论推导ADePT的分析错误，也记录一下关于differential privacy 的一些背景知识和推理<br>

​		从高层次的角度来看，DP使用的是个人的概念，其信息保存在数据库（数据集）中。每个人的数据点（或记录）可以是一个位、一个数字、一个向量、一个结构化记录、一个文本文档或任何任意对象，都被认为是私有的，不能公开。此外，即使数据库中是否有任何特定的个人，也被认为是私有的。<br>

**Definition 2.1** **隐私保护对象的定义**<br>

![image-20220320204003622](C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320204003622.png)

**Definition 2.2** **随机算法的定义<br>**

<img src="C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320204014585.png" alt="image-20220320204014585" style="zoom:50%;" /><br>

**Definition 2.3** **DP隐私保护机制的定义：输出的越像，保护得越强<br>**

 ![image-20220320204031569](C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320204031569.png)

​		可以从上式得到（或者理解为Differential Privacy这件事情要做的就是让原本的样本x和y更相似<br>

![image-20220320204125194](C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320204125194.png)<br>

**Definition 2.4 ** **L1敏感度定义**<br>

![image-20220320204236164](C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320204236164.png)<br>

**Definition 2.5**（Laplace概率密度定义）<br>

<img src="C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320204318683.png" alt="image-20220320204318683" style="zoom: 50%;" /><br>

**Definition 2.6**（在原始映射中添加Laplace噪声，也就是开始**加噪**）<br>

<img src="C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320204447841.png" alt="image-20220320204447841" style="zoom:50%;" /><br>

**证明**接下来是证明使用Laplace加噪后的函数满足<img src="C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320204547143.png" alt="image-20220320204547143" style="zoom:50%;" /><br>

<img src="C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320204614597.png" alt="image-20220320204614597" style="zoom:50%;" /><br>

回顾 ADePT中的裁剪函数（对应Definition2.6）<br>

<img src="C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320205143822.png" alt="image-20220320205143822" style="zoom:50%;" /><br>

<img src="C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320205233868.png" alt="image-20220320205233868" style="zoom:50%;" /><br>

假设:<br>

<img src="C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320205424088.png" alt="image-20220320205424088" style="zoom: 50%;" /><br>

<img src="C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320205432424.png" alt="image-20220320205432424" style="zoom:50%;" /><br>

其中问题在于：		$\Delta f$ 不是2C，而是$2C\sqrt{n}$，并不满足$(\varepsilon,0)$-DP<br>

补救方法：<br>

1. 潜在矢量剪裁可以使用“1-范数”<br>
2. 拉普拉斯噪声可以使用确定的正确灵敏度$2C\sqrt{n}$<br>

但是因为两种方法都会导致噪声增加，使得任务的精度变差。<br>

by *Meizhi Zhong*<br>



### LDP-DL

**Title：**Locally Differentially Private Distributed Deep Learning via Knowledge Distillation<br>

**Author:** Di Zhuang, Mingchen Li and J. Morris Chang<br>

**动机：  ** 数据使用者（训练模型的人） 希望使用跨多个不同数据所有者分离的数据构建DL模型。然而，由于数据的敏感性质，这可能会导致严重的隐私问题，因此数据所有者会犹豫不决，不愿参与。<br>

**亮点：  **（和SeqPATE类似，换了一种故事）<br>

1. 一个新颖、有效和高效的**隐私保护分布式深度学习框架**，使用局部差异隐私和知识提取<br>
2. 一种主动抽样方法【**ActiveQuerySampling**】，减少从Data User到每个Data Owner的查询总数，从而降低隐私预算的总成本。（就是学生模型置信度低的预测对教师进行Query）<br>

**模型：   **作者提出了LDP-DL（Locally Differentially Private Distributed Deep Learning），这是一个通过**局部差异隐私**和**知识提炼**来保护隐私的分布式深度学习框架，其中**每个Data Owner使用自己的（局部）私有数据集学习一个教师模型，Data User学习一个学生模型来模拟教师模型集合的输出**。<br>

<img src="C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220318104020490.png" alt="image-20220318104020490" style="zoom:50%;" />

**蒸馏框架**<br>

<img src="C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220318135125788.png" alt="image-20220318135125788" style="zoom:50%;" /><br>

**知识蒸馏的损失函数**<br>

- 使用多个温度<br>

![image-20220320210545107](C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320210545107.png)<br>

**实验**<br>

*数据集： CIFAR-10; MNIST; Fashion-MNIST* <br>

*baseline: DP-SGD;PATE;DP-FL*<br>



### RGP

**Title：**Large Scale Private Learning via Low-rank Reparametrization<br>

**Author:** Da Yu ; Huishuai Zhang ; Wei Chen;  Jian Yin;  Tie-Yan Liu<br>

**亮点**<br>

1. 第一个能够在BERT模型上应用**差异隐私**<br>
2. 在四个下游任务GLUE(MNLI, QQP, QNLI, SST-2)中，$\epsilon=8$的平均准确率为83.9%<br>
3. 提出了重新参数化梯度扰动（RGP），当在**大型模型上应用DP**时，可以降低内存成本并提高实用性。<br>



**动机**<br>

​		提出一种**重新参数化**方案（类似高效率参数），以应对在**大型神经网络**上应用**DP-SGD**所带来的问题：1）**存储**每个梯度的巨大**内存**成本，2）增加的噪声随着维数增加而增加。<br>

**模型**<br>

【改变权重矩阵更新的方式，并加噪声】<br>

​		reparametrized 保持了【forward/backward】过程不变，使得可以在**不计算梯度**的情况下计算投影梯度。为了使用差分隐私进行学习，作者设计了**重新参数化梯度扰动**【reparametrized gradient perturbation】（RGP），扰动梯度载波矩阵上的梯度，并根据**噪声梯度**重建原始权重的更新。<br>

![image-20220320212546464](C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220320212546464.png)<br>

权重矩阵更新的细节如下：<br>

在每次更新时，对于具有权重矩阵W的层，RGP包括四个步骤：<br>

- 1）生成梯度载波矩阵L和R<br>
- 2）运行重新参数化的forward/backward过程并获得各个梯度$\{∂_iL\}^m_{ i=1}$和$\{∂_iR\}^m_{ i=1}$<br>
- 3）剪裁并扰动梯度（和梯度下降非常像）<br>
- 4）在原始权重矩阵上重建近似的梯度<br>

**Step1** ：**选择“好”“梯度载波矩阵**，使重建的梯度尽可能接近原始梯度。<br>

​		首先，这要求给定秩r，生成的梯度载波矩阵应与原始梯度的**主分量对齐**。此外，要在步骤4）中重建梯度，需要梯度载体具有**正交的列/行**。<br>

​		因此作者提出：<br>

1. 使用**历史更新来查找梯度载波**。由于DP的后处理特性，历史更新不敏感。<br>
2. 应用Gram-Schmidt过程对L和R进行正交归一化<br>

<img src="C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220308203642877.png" alt="image-20220308203642877" style="zoom:50%;" /><br>

**Step 2**<br>

​		重新参数化和一轮forward/backward传播<br>

**Step 3**<br>

​		提供隐私保护的步骤<br>

​		每个梯度$\{∂_iL \ ∂_iR\}^m_{ i=1}$首先通过预定义的阈值进行**剪裁**。然后，将高斯**噪声添加**到聚合梯度中，以建立差分隐私边界。添加的噪声的能量与维数成正比（载波矩阵的秩r）。因此，为了使噪声能量较小，应该使用较小的秩r。然而，较小的秩会增加步骤1）中的近似误差。（tradeoff的体现）<br>

**Step 4**<br>

​	使用加噪后的梯度来重建原始权重的梯度。然后，任何现成的优化器都可以使用重建的梯度<br>

**实验**<br>

*数据集：MNLI, QQP, QNLI, and SST-2 from GLUE*<br>

![image-20220308143503936](C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220308143503936.png)<br>

- 为什么RGP-random比RGP差？<br>

因为随机子空间不能像历史更新的子空间那样有效地捕捉梯度信息。（如何寻找梯度矩阵那一节）<br>

- 为什么单纯的DP-SGD【2016】最差？<br>

因为噪声压倒了梯度中的有用信息<br>

![image-20220308143353657](C:\Users\zhongmeizhi1\AppData\Roaming\Typora\typora-user-images\image-20220308143353657.png)<br>











### Back Translation and Translationese

<b>Title</b> Understanding Back-Translation at Scale (Edunov-18)<br>
<b>简介</b> 18年facebook的文章，探讨了BT使用时一系列的问题, 在WMT-14-英德翻译中达到SOTA (35 BLEU)<br>
<b>背景</b> BT问世以来在各方面都有很多应用，除了作为利用target端单语数据的手段，也可以利用source端的单语数据，或通过dual-learning进行无监督翻译[1] <br>
<b> 方法、实验与分析 </b> <br>

- 进行BT时的解码算法：最初论文中使用beam search, 也有论文[1]直接使用greedy search. 这些方法都是对最大输出概率估计(maximum a-posteriori)的近似，但是在这些解码算法在不确定性较强的文本生成任务，如对话、故事生成等，所以作者比较了不同的解码算法：greedy/beam=5/sampling（依照输出概率分布进行采样）/top10(在top10个词中采样)/beam+noising(参考[1]，对于source句子，以0.1概率删除词, 0.1概率用filler替换词，交换词)。实验这些解码算法生成的BT数据对于NMT训练的作用(3M-6M-12M-24M)<br>
 ![](img/2022-03-2.png)<br>
- 以上结果表明，sampling/beam+noise的效果最好，为什么呢？作者猜想，是因为这两种方法在BT中可以生成更多样化的source文本，在后续NMT的训练中可以提供"richer training signal”<br>
- "richer training signal”：	作者做实验验证不同解码算法生成的数据在训练时的交叉熵（这里是perplexity），发现使用sampling/beam+noise的perplexity相对比较高，且与最终的模型精度成正相关. 所以作者认为，training loss高，代表模型更难拟合训练数据，训练过程中需要处理更多样化的source文本，就是所谓的richer training signal<br>
- low resource: 实验发现，在低资源场景下，用beam search生成BT效果要优于sampling/noise 因为本身双语训练数据较少，加了噪声的BT数据对训练的负面影响会更突出<br>
![](img/2022-03-3.png)<br>
- domain的影响，以及与真正平行语料的对比：实验发现（这里的BT解码算法采用sampling)，当BT数据的domain与验证集domain不一致时，效果不太好，但是一致时，BT数据与真实平行语料的差距极小.<br>
![](img/2022-03-4.png)<br>
- 对bitext上采样: 将BT数据与bitext混合训练时，对bitext进行上采样。结果是beam/greedy性能有所提升，其他解码方法性能有所下降，这还是优于刚才提到的training signal问题，通过上采样使得前两种解码方法生成的数据中也有了足够的training signal


这篇文章没有提到translationese这个词，但是全文所说的东西，尤其是有关beam+noise/sampling与beam/greedy对比的实验结果，以及training signal这个概念的讨论与translationese息息相关。可以看出beam生成的BT数据之所以不好，是因为生成的source文本中含有translationese，导致翻译过程过于简单，模型太轻易地fit训练数据

[1] [Unsupervised Machine Translation Using Monolingual Corpora Only] (https://openreview.net/forum?id=rkYTTf-AZ&utm_campaign=piqcy&utm_medium=email&utm_source=Revue%20newsletter)

by *Baopu Qiu*

<b>Title</b> APE at Scale and its Implications on MT Evaluation Biases (WMT-19-Google)<br>
<b>简介</b> 这篇论文其实是translationese as multilingual BERT的前身，从文中把APE当作一个翻译模型的思想，以及RTT的使用就能看出来,同时也引用了edunov论文中noise BT的做法；同时作者还说明了translationese对于翻译评价的作用，作者呼吁使用更高质量的测试集或使用multi-reference BLEU, 虽然后者被reference not innocent那篇论文证明并不行.<br>
<b>方法</b> 就是用双语数据去训练一个RTT模型，然后用这个模型处理单语数据生成APE的训练数据，训练APE模型.<br>
![](img/2022-03-5.png)<br>
![](img/2022-03-6.png)<br>
<b>实验</b><br>
- 使用bitext训练RTT模型，生成单语数据，训练APE模型，对于EN-DE NMT的后编辑效果如下：<br>
![](img/2022-03-7.png)<br>
- 发现对于某些年份的BLEU值下降了，进一步研究发现，对于de-origin的效果一直在提升，对于en-origin的效果一直在下降，所以如果只对de-origin的使用译后编辑，会发现BLEU值稳定提升.<br>
- 除了对标准NMT模型（用bitext训练）能带来提升以外，对于用bitext+NBT(这里的BT引用edunov论文中beam+noise的生成方法)训练的NMT模型也能带来提升<br>
- 并且作者认为，BLEU值下降了不是因为翻译质量下降，而是因为reference本身有问题，所以组织了人工评价，发现fluency和accuracy都有提升<br>
- 作者还发现，训练APE没必要用那么多单语数据(216M)，仅用4M，就可以达到差不多的效果<br>
- 将APE应用到多个MT系统、多个语言对中，发现都有提升<br>
- 实验说说明迭代式APE并不能带来进一步的提升; 而训练的reverse APE会对de-origin的结果带来极大负面影响，进一步说明了APE模型“能够将translationese改写成natural text"的作用<br>
- 对于RTT模型的输出统计说明，translationese所含的词汇总数少于original text，而APE模型可以在一定程度上丰富词汇量<br>

by *Baopu Qiu*


<b>Title</b>  Tagged Back-Translation (WMT-19-google)<br>
<b>简介</b> 也是 Translationese as a Language in “Multilingual” NMT的前身，主要对edunov18年论文所提出BT中beam+noise的作用这一点提出反驳. 他认为beam+noise之所以有用不是因为noise使得source端变得多样化，而是这种方式”提示“模型当前训练数据是BT生成的，所以作者用了另一种打标签的方法作为提示，然后发现能得到与beam+noise相当的效果。同时作者还通过研究attention matrices进一步验证了自己的结论<br>
<b>实验与分析</b>作者用来对比的对于BT数据的不同处理方式：<br>
![](img/2022-03-8.png)<br>
- 对于**双语平行语料**， 作者测试了不同的加噪比例，发现即使加到百分之百，BLEU值也妹有下降很多（这里有疑问？双语语料都加到100%噪声了还没影响？不信，不过这里的100%的意思是，100%概率按照edunov论文中的方式加噪，噪声也不是很多）<br>
- 作者发现taggedBT跟noiseBT总体性能差不多，总的来说，资源越低(EN-RO),taggedBT越好<br>
- 作者发现，taggedBT有一个优势就是可以用到iterative-BT中，实验表明iterative-3效果是最好的，作者猜想这是因为tag的出现显式地区分了BT数据与bitext数据.<br>
- 对注意力矩阵的分析:<br>
	1. 定义一个函数$ASR_j$ 代表source中第j个token上的注意力系数，<br>
	2. 计算注意力矩阵的交叉熵<br>
	得到结果如下:<br>
	![](img/2022-03-9.png)<br><br>
	可以发现taggedBT有更高的ASR，因为target中所有token都给source中的第一个词很高的注意力；另一方面来说，BT的交叉熵是最低的，这暗示着BT数据会带来的word-for-word偏置，即输出中翻译腔的情况会比较严重。<br>
- 解码的时候反向添加tag，发现taggedBT和noisedBT的性能都下降，证明的tag或者noise的用处，不过，作者并不知道为什么相对于noise，tagged的性能下降幅度没有那么大，一个有趣的细节是在en-de翻译中，tagged生成了更多的英文句子<br>
	

<b>失败的结果与展望</b> 作者还报告了一些失败的结果, 在FT中使用tag并没有带来什么提升<br>

by *Baopu Qiu*


<b>Title:</b> Tagged Back-translation Revisited: Why Does It ReallyWork? (2020acl-NICT)<br>
<b> 简介</b> 这篇文章所讨论的内容与上一篇几乎完全一样，实验也差不多，就是细节上稍微有些差别，主要包括三个问题：<br>
Q1：BT训练会不会带来translationese问题？答：会<br>
Q2: 加入Tag能否解决这一问题？答：可以，从文中的图就可以看出<br>
**Q3**：Tag对与低资源和高资源的影响，这里得出了**跟google论文相反的结论**，即在低资源场景下BT的效果很好，加上tag没什么用处。他们和Google的区别是，Google是在低资源的语言对上做实验，而他们是在高资源语言对上采样200K个句对，构造了一个低资源场景，然后发现BT在任何情况下都有用，他们认为这是因为低资源场景下的BT模型本身效果就不怎么样，相当于被动加入noise，（注：因此就有了beam+noise的效果）<br>
Q4：给test set添加tag/反向添加：与Google相同的结论, 这样会大幅降低模型性能，证明tag确实对NMT model产生了重要的影响<br>

by *Baopu Qiu*<br><br>


<b>Title:</b> BLEU might be Guilty but References are not Innocent-EMNLP-2020-Google<br><br>
<b>背景</b> 当前基于参考译文的翻译自动评价指标难以分辨top系统之间的细微差别。作者发现问题不仅在于指标本身，参考译文也有问题：缺少多样性，*翻译腔*，这导致自动评价指标会给与reference句式类似的翻译输出打较高的分数。<br>
1. 组织专业人员，人工生成参考的paraphrase.发现多样化的参考译文减轻了以上问题，并且在对利用BT数据训练/APE得到的MT输出的自动评价中，与人工评价有了更好的相关性
2. 作者发现BLEU中的多参考(multi-reference)的效果不明显，于是设计了另一种利用多参考的方法——在所有参考中选取忠实度(adequacy)最高的一个作为单参考使用，以更好评价top系统之间的差别
3. 开源数据集

<b>细节</b><br>
寻找专业翻译公司人员，首先让10个母语者、语言专家在没有任何CAT辅助工具的情况下为EN-DE编写reference，（AR)再由以下规则生成尽量多样化的paraphrase（AR-para、WMT-para)<br>
![](img/2022-03-10.png)<br>
有人可能会质疑，这样做会不会降低参考的忠实度，所以作者也专门对忠实度进行人工评价<br>
<b>实验</b><br>
- 首先是人工忠实度评价的结果，WMT reference的忠实度与专家编写的reference(文中称为AR)差不多，专家paraphrase的reference的忠实度有所降低。但是如果挑出对于每个句子忠实度最高的reference(下文中的HQ())，还是有接近90的忠实度。<br>
- 在自动评价指标仍然使用sacreBLEU的情况下，但是参考换成人工编写的paraphrase，与人工评价的相关度就有了一定提升，结果如下<br>
![](img/2022-03-11.png)<br>
- 由下图可以看出，使用paraphrase结果作为参考译文，对于top系统的评价效果有了极大提升<br>
![](img/2022-03-12.png)<br>
- 重新评价WMT2019机器翻译的系统，发现使用作者给出的参考译文（包括AR,paraphrase）时，Facebook系统的BLEU值都重回第一（Facebook人工评价是第一，但是原始BLEU值低于很多其它系统）<br>
![](img/2022-03-13.png)<br>
- 对于BT/APE的评价：BT/APE操作可以使得NMT系统生成更natural的输出，但是使用当前的reference计算BLEU值无法体现这一点。如果使用paraphrase过的reference，可以捕捉到BT/APE对于翻译质量的提升<br>
- 这些reference用在其他评价指标中，包括chrf/yisi/bertscore，都可以带来提升<br>
- 测试集中Translationese的出现，主要是因为人类在翻译的时候也倾向于几个词几个词地翻译。作者用FastAlign工具生成词对齐，再统计此词对齐中位置的差值，发现paraphrase过的reference这一差值明显高于原始reference，证明paraphrase会生成更多样化的句式，与源语言差别更大.<br>

by *Baopu Qiu*<br><br>

<b>Title:</b> Human-Paraphrased References Improve Neural Machine Translation（WMT-20-Google）<br>
<b>简介</b> 主要是对上一篇文章的应用.既然专家编写的reference可以提升BLEU指标的可信度，那么可以在训练NMT模型的时候都参照BLEU-P指标进行模型选择、微调和集成等等，文中实验证明这种方法确实带来提升.<br>
<b>其他</b> <br>
这篇文章的related work写得比较全面，涉及到<br>
1. paraphrase的历史，有人用paraphrase做MT的数据增强，但在评价paraphrase的时候却使用标准的reference，于是就出现一个问题：随着与原句的overlap减少para的质量也会下降，不适合做翻译的数据增强. 所以在MT的paraphrase中，要遵从freitag提出的“尽可能多的para"理念<br>
2. MERT ## [Minimum Error Rate Training in Statistical Machine Translation](https://aclanthology.org/P03-1021.pdf) <br>
3. 有关translationese，对于评价的影响(Toral18 Toral19 graham19)，以及reference的影响(fomicheva16 maqingsong 17), 对于训练过程的影响（三人组20，rico19）
同时也提到了开发NMT系统中的一些流程，涉及到一些概念，包括
1. 数据清洗的CDS工具
2. BT中需要筛掉那些错误的语言
3. MT的ensemble--ensemble decoding： 解码时投票法选择target word，这使得结果会是common average language，在这篇文章中从BLEU-P的角度评价，看不到太多提升
4.  reranking：借用[Simple and effective noisy channel modeling
for neural machine translation](https://arxiv.org/pdf/1908.05731.pdf) 这篇文章的做法，对所有top-50beam的结果进行重排，但与论文中不同的是使用MERT做微调

by *Baopu Qiu*<br><br>

<b>Title</b> Attaining the Unattainable? Reassessing Claims of Human Parity in Neural Machine Translation (WMT-18-Toral)**数据集**<br>
<b>简介</b> 早期工作，是比较早研究翻译腔问题的文章，结论都是众所周知的，实验设置、写作也比较普通。从”机器翻译是否达到了人类的水平”这一问题入手，结论包括：<br>
1. Test set中target-origin和source-origin对于评价的影响<br>
2. 人工评价应该找专家来评价，专家之间的一致性更高，辨别能力更强<br>
3. 评价的时候应该考虑上下文<br>

其中第二第三点其实启发了Google后面的那篇文章(Experts, errors....)<br>

by *Baopu Qiu*<br><br>

<b>Title</b> The Effect of Translationese in Machine Translation Test Sets (WMT-19-Toral)<br>
<b>背景</b> 早先有研究发现，使用Original source -> Translated target训练的MT系统性能比反过来要好. 也有研究发现，当source 为original时，人类翻译的质量明显高于MT翻译的质量，但是当source为translationese时就没那么明显。这篇文章以WMT16-18所有语言对的所有提交系统为研究对象，研究translationese在test set中的影响<br>
<b>内容</b> 主要研究了三方面的内容<br>
1. 实验证明，测试集中 original source->trans-target 的人类评价得分明显低于 trans-source->orginal-target<br>
![](img/2022-03-14.png)<br>
2. 实验证明，这一差异影响到了relative ranking的结果<br>
![](img/2022-03-15.png)<br>
细节1：里面的编号是指的cluster id，进行显著性测试，把没有显著差别的系统归类到1个cluster中<br>
with/without ties: 所谓ties是指哪些在同一个cluster里，但是DA分数不同的系统，with ties表示即使位置改变，只要还在一个cluster里就算不变. 结果without ties的情况下，测试集数据类型给测试结果带来的变化更大<br>
3. 寻找translationese的影响与语言对之间的联系. 作者提出两个方面:<br>
i) 翻译腔的影响与翻译质量有关-实验证明，翻译质量越高（DA)的语言对受翻译腔的影响越小），大概是因为翻译质量低的语言一般是低资源语言，而低资源语言的训练数据中可能有更多的合成数据，因此在测试集中original source->trans-target的表现会下降得更多<br>
ii) 翻译腔的影响与语言之间的相似度有关-实验发现并没有什么关系<br>
by *Baopu Qiu*<br><br>

<b>Title</b> Statistical Power and Translationese in Machine Translation Evaluation（Yvette graham-emnlp-2020）**数据集**<br>
<b>简介</b> 这篇文章所讨论的问题以及使用的方法跟上面Toral的两篇文章很像，但是里面图片都画的很好看,主要讨论的问题有：<br>
1. 还是有关test set 中translationese对于结果影响的问题<br>
	![](img/2022-03-16.png)<br>
	从DA的角度评价，再一次证明了翻译腔对结果的影响<br>
	![](img/2022-03-17.png)<br>
	从BLEU的角度评价，作者发现这种影响是系统性的，不太影响系统相对之间的排名（**这与Toral文章的结论相反**)<br>

建议在testset中移除translationese的部分<br>
2. 人工评价有一些问题，应该统计有多少个*不同的*句子（图中的n)被人工评价了，而且要考虑document context<br>
![](img/2022-03-18.png)<br>
3. 显著性测试，避免II类错误：应该拒绝假设，没拒绝，显著性测试结果表明，出现了很多平局(tie)的现象，但是这并不代表机器翻译就是与人类同一水平了，还是测试机太弱的原因。<br>

by *Baopu Qiu*<br><br>

<b>Title</b> On The Evaluation of Machine Translation Systems Trained With Back-Translation(edunov-acl2020)**数据集**<br>
<b>简介</b> 这篇文章与以上文章的不同点在于，开始探讨*如何解决翻译腔对现有自动评价指标的负面影响*这一问题，(之前的论文只是说有翻译腔这个东西，为了解决翻译腔，所以就要用人工评价），提出将LM score作为BLEU以外的辅助指标，评估流畅度<br>
<b>背景</b> 提到了Toral的论文。也提到Yvette2019的论文，但是认为Yvette论文中“target-origin更好翻”这一结论是不严谨的，因为source-origin和target-origin本身的内容就不同，有可能source-original的内容本来就不好翻（这一点在google acl2020的论文中也有说明）<br>
<b>方法</b><br>
1. 虽然批评了Yvette2019的实验方法，但是同意先前论文的基本结论。即"translationese更好翻译“. 作者构造了四种类型的数据<br>
![](img/2022-03-19.png)<br>
以上对比，遵循先前工作的实验思路，发现从BLEU的角度看，BT确实对target-orginal的翻译有很大提升，而source-origin没有<br>
![](img/2022-03-20.png)<br>
作者认为比较合理的实验思路，发现将source-original换成合成的translationese-original时，不管用不用BT，BLEU值都有了提升<br>
2. 引入人工评价，先前工作，比如Google三人组的一系列工作，都是在指出有翻译腔这个问题（一般会把它归咎于BT）之后，提出某种方法”解决“翻译腔，然后再提出BLEU值无法很好地评价自己的成果，于是引入人工评价，用人工评价证明自己提出的方法在有/无翻译腔的情况下都很好。而这篇文章的作者发现，引入人工评价之后，即使**不用任何其他方法**, 单纯评价BT，也会发现BT在有/无翻译腔的情况下都很好。<br>
3. 从2中结论出发，作者研究BLEU为什么无法区分OP和BT的结果，作者发现当进行source-original到target-translationese的翻译时，不管OP和BT的结果都会有翻译腔；而进行source-translationese到target-origin的翻译时，相比OP,BT可以更好地降低perplxity，所谓的”更加流利、更加自然“，专家们对流利度的评价也证明了这一点<br>
4. 从3中结论出发，作者提出应该用LM Score作为BLEU的补充，度量翻译的流利度<br>

<b>反思</b> 这篇文章对于相似结论的证明，使用了更严谨的实验方法，值得学习。另外BT人工评价的结果，一定程度上也质疑了先前很多声称自己”可以改善translationese问题“的工作。作者尽管尝试用LM score改善BLEU评价，但给人的感觉还是比较trivial.<br>

by *Baopu Qiu*<br><br>

<b>Title</b> Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance （ACL ARR 2021) **数据集**<br>
<b>简介</b><br>
这篇看起来像是NICT三人组的工作，主要考察了MT中的以下问题：<br>
data-model-alignment: 模型翻译的方向与训练数据的方向是否相同<br>
train-test alignment: 训练集和测试集的方向是否相同<br>
![](img/2022-03-21.png)<br>
作者自己做了数据集，然后在自己的数据集上分析，得到以下结论：<br>
Q1. 先前工作认为test数据的方向影响MT的性能，翻译翻译腔文本很简单，作者认为并不是这样（又挑战先前结论），因为简单与否还跟train-test alignment有关.<br>
Q2. BT的效果也与test-model alignment有关（该结论与先前大部分工作一致）<br>
Q3. 即使是对source-origin来说，BT仍然可以带来性能提升，只要BT的方向与test set一致（疑问，那这里就不能叫BT了呀，应该叫FT，*这里应该是作者表达有误*）<br>
Q4. data-model-alignment的影响

<b>方法细节</b><br>
数据收集: 与Riley(google-acl2020）等人一样也遇到了翻译方向的标注问题（只有test set有标注, train set没有），作者以EuropaPara为基础语料，应用一定规则在训练集上筛选出可以自动生成翻译方向标签的句对，具体规则如下：<br>
*筛选出那些有提示源语言信息的句子对，记录这些句子的说话人(speaker)，把这些speaker所说的所有该语言的句子标记为origin*<br>
有关Q1：在控制其他变量不变的情况下，调整训练集中source-origin的比例（图中的 $\alpha\%$ ) , 分别测试在source-origin和target-origin的测试集中的性能.<br>
![](img/2022-03-22.png)<br>
可以看出，test与train的数据方向越一致，性能越好. 所以并不应该把target-origin的测试集移除（反对先前工作），而是分别报告train和test的方向，并且在MT训练中尽量使用方向一致的数据<br>
有关Q2、Q3：使用source端单语数据的ST与使用target端单语数据的BT，在方向与测试集方向一致时，可以带来较多的提升（与先前所有工作的结论相同）<br>
有关Q4：这篇文章没有人工评价的部分。作者引入了因果推断(causal inference)的一些概念，进一步说明data-model alignment 对 MT performance的影响。之前的论文之所以结论细节上有很多出入，是因为没有翻译方向以外的变量，比如语料本身的内容对MT performance的影响，作者画出一个因果关系图: <br>
![](img/2022-03-23.png)<br>
因此可以计算平均处理效应值（ATE)<br>
![](img/2022-03-24.png)<br>
其中Z代表其他变量，在本文实验中包括句子长度和句子内容. 作者通过协变量匹配方法，寻找正向与逆向中长度相同、语义相同的句子对（语义相似度用sentenceBERT句子向量的余弦相似度来度量），在这些句对上计算BLEU差值的期望.<br>

<b>总结与反思</b> 得到的结论与前面论文基本相同，引入因果推理的思路比较fancy，看起来(比之前论文）更严谨一些，可以学习。但是，对于MT performance的影响因素仅限于作者给出的图中那几点吗？因果推理的引入有一点刻意的感觉.<br>

by *Baopu Qiu*<br><br>

---

### 数据集对于NMT训练过程的影响

<b>Title</b> Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation (Rico-2021-ACL)<br>

by *Baopu Qiu*<br><br>

---

### Hallucination

<b>Title</b> On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation (Rico-acl2020)<br>
<b>简介</b> 本文首先指出，NMT模型在领域转换(domain shift)时出现的幻觉现象是Exposure Bias导致的，文中的实验结果也证明了这一点；并且提出将MRT作为训练目标可以一定程度上缓解幻觉现象.<br>
<b>方法与实验</b><br>
<b>Minimum Risk Training：</b> NMT的目标函数通常是MLE（极大对数似然），这里引入最小训练风险<br>
![](img/2022-03-25.png) 这里的 $\Delta(\hat{y},y)$度量模型输出与ground truth之间的不一致性 ，而后验概率$y(x)$通过采样翻译输出，并计算![](img/2022-03-26.png)得到，其中$\alpha$是控制子空间sharpness的超参数。训练过程中先以MLE为目标进行训练，再用MRT进行微调，在IWSLT14上的结果如下：<br>
![](img/2022-03-27.png)<br>
幻觉现象主要出现在OOD(out of domain)的翻译中，因此作者使用了[1]中包含五个领域的数据集，用其中医药的数据集做训练和域内测试集，其他做OOD测试集，还组织人工评价翻译的”忠实度“，结果如下：<br>
![](img/2022-03-28.png)<br>
![](img/2022-03-29.png)<br>
主要的对比方法是[1]这篇文章给出的baseline，图中第三行都是[1]中使用的缓解翻译幻觉的方法，从图中看来，label smoothing似乎比文中提出的MRT方法更有效，而且作者的方法效果都没有超过[1].

<b>不确定性分析</b> 作者对比了在每个time step模型对于reference token和distractor token给出的概率，图中左边两张图是in-domain结果，右边是OOD<br>
![](img/2022-03-30.png)<br>
最直观的感觉是MRT会随着训练进行不断增加概率，但作者想说明的其实是，相较于MLE, MRT对reference token给出的概率更高，distractor更低(图中仔细观察才能看出来，感觉这是一个不太好的地方)<br>
<b>beam size</b> 实验表明，增大beam size有时确实会降低模型性能（exposure bias），这很大程度上是由于幻觉现象的产生，而MRT可以缓解这一问题

<b>总结</b> 感觉这篇文章的贡献不是很大，仅在于确认了一些已有结论（幻觉与Exposure bias), 给出的解决方案(MRT)效果也不是很好.<br>

<b>相关工作</b><br>
[1]  [Domain Robustness in Neural Machine Translation](https://aclanthology.org/2020.amta-research.14.pdf)

by *Baopu Qiu*<br><br>

<b>Title</b> The Curious Case of Hallucinations in Neural Machine Translation(2021-naacl,好文章)<br>
<b>简介</b> 本文首先对幻觉现象做了完整的定义与分类，并尝试说明不同类型的幻觉现象可能是由什么原因造成的。对于扰动下的幻觉现象，应用MVE[2]方法，证明那些most memorized samples在扰动之下更容易产生幻觉现象；对于自然产生的幻觉现象，作者认为是训练集的噪声导致，并设计实验研究何种类型的噪声导致了何种类型的幻觉。<br>
<b>方法与实验</b><br>
**幻觉的分类**：作者认为机器翻译中的幻觉现象可以分为两种：<br>
1. 在source端添加扰动的情况下出现的幻觉，添加扰动使得输出有很大变化，这一问题往往在robust NMT[1]的研究中提到<br>
2. 自然出现的幻觉(Natural Hallucination, NH)，source端没有扰动，但是输出出现low adequacy high fluency的问题（上面文章提到的就是这种）<br>
    而进一步，NH也可以分为两种, detached Hallucination (DH), 代表那些不忠实的翻译，oscillatory Hallucination(OH), 代表输出中有很多重复字符串的翻译。[1]中还提到了输出过短、输出中直接出现source中的句子的现象，但是这篇文章没有研究.<br>

**扰动下的幻觉**:  论文[2]提出，在数据呈现长尾分布的情况下，标签记忆(label memorization)有助于提高深度学习模型的泛化性，而论文[3]进一步提出了一种估计每个样本在训练过程中的记忆程度的算法(memorization value estimation, MVE)，本文就借用了[3]的估计算法，为了适用于NMT任务，作者把算法中的metric换成机器翻译的metric，如BLEU/CHRF，算法如下：<br>
![](img/2022-03-31.png)<br>
对数据集中所有样本计算MVE以后，选出其中最大的100个，同时随机选取另外100个样本，作为对比实验。具体来说，借用论文[1]中的扰动算法，在这两个数据集上添加扰动，尝试生成幻觉输出. 其中token set T由整个数据集中出现频率最高的100个token中随机选30个组成。<br>
![](img/2022-03-32.png)<br>
然后人工评价统计成功生成幻觉的数量，结果如下：<br>
![](img/2022-03-33.png)<br>
可以看出MVE值较大的样本，更容易被扰动，为什么呢？作者分析了NMT在这两个数据集解码阶段最后一层的注意力矩阵，发现memorized样本的注意力交叉熵比较低，对于最后一个token的attention又比较高（感觉也没分析出什么，之前翻译腔的分析也是说翻译腔交叉熵比较低，容易一一对应）<br>
**自然幻觉**: 为了考察数据集中的噪声类型与幻觉类型的联系，作者构造了IRS数据集：从WMT英德中随机选21个source句子和不对应的target组成句子对。在此基础上，作者构造了4种类型的噪声：<br>
1. UU: 从WMT中采样21K个source，对每一个source随机选一个target组成句子对
2. RR：将IRS数据集复制1000次
3. RU：将IRS中的source复制1000次，为每一个在WMT中随机选取一个target组成句子对
4. UR: 将IRS中的target复制1000次，为每一个在WMT中随机选取一个source组成句子对

作者使用IWSLT14英德+上面方法构造的21K个句对的噪声数据做训练集，分别在1.IWSLT测试集 2. IRS 3.VRS(source和IRS一样，但是为每个source找*正确*的target配对) 这三个数据集上考察1. BLEU值  2. IRS中的NH比例 3. IRS中OH比例 4. IRS上的NMT模型输出和IRS测试集中的reference完全一样的句子的比例（IRS-repeat） 5. 对于IRS的预测结果中不重复的bigram数量，结果如下：<br>
![](img/2022-03-34.png)<br>
可以看出 1. 总的BLEU值变化不大 2.RR完全“记住”了IRS测试集中的句对 3. UR中出现相当比例的irs repeat （但事实上训练过程中模型并不可能看到IRS中的source，下方注意力图显示，加入UR噪声使得模型解码时，对于source的注意力集中在最后一个token，这与上一节对于memorized set的分析**结论一致**）4. RU导致了很多OH，从对于top5bigram的统计也可以看出，RU导致翻译输出集中于高频词的重复

**BT/KD与翻译幻觉**: 作者讨论了常用数据增强方法是否会产生翻译幻觉。上面所有对幻觉的识别都是人工完成的，但是作者根据幻觉的特点，设计了一个heuristic的自动识别幻觉的算法，如下<br>
![](img/2022-03-35.png)<br>
F1: 那些target中top-repeated ngram数量大于source中的translation的数量
F2: 某些翻译输出中一个翻译对应多个不同的source，统计他们的数量
S_epsilon: 假如epsilon=25% 那么这个变量是指，对所有句对计算语义相似度，排后25%的那些
实验结果如下：<br>
![](img/2022-03-36.png)<br>
<b>总结</b> 这篇工作比较全面且严谨，引入长尾分布相关理论使文章更加fancy，而实验分析噪声与幻觉的关系，可以为我们提供后续思路——可以根据出现的幻觉类型有针对性地进行数据筛选与增强(比如上采样长尾分布的部分)

<b>相关工作</b><br>
[1] [Hallucinations in Neural Machine Translation](https://openreview.net/pdf?id=SJxTk3vB3m)<br>
[2] [Does learning require memorization? a short tale about a long tail](https://dl.acm.org/doi/10.1145/3357713.3384290)<br>
[3] [What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation](https://papers.nips.cc/paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html) <br>

by *Baopu Qiu*<br><br>

<b>Title</b> Q2: Evaluating Factual Consistency in Knowledge-Grounded Dialogues
via Question Generation and Question Answering(2021-EMNLP)<br>
<b>简介</b> 借用文本摘要中的事实一致性检验方法，提出一种*适用于开放域对话的、不需要reference的* 事实一致性自动评价方法. 具体来说, 先基于response中的informative span，进行自动问题生成(QG)，然后再用生成好的问题在知识库中做问答(QA)，把得到的答案与response中的span对比（本文采用NLI模型），衡量内容的一致性，给出评价。<br>
除此之外，作者还给出了一个数据集，实验结果表明所提方法效果很好.<br>
<b>方法</b><br>
输入：历史会话h, 当前回复r，知识库k, 目标是评价当前回复与知识库的事实一致性
**问题生成QG**: 使用Spacy找到r中所有的*命名实体*和*名词短语* a，将每个Span以及r作为QG的输入，生成问题q. QG使用在SQUAD1.1上微调的T5模型，每次生成5个，然后进行问题筛选——对于每个生成的q，把它和r作为QA模型的输入，查看找到的答案是否与a相同，如果不同就删去q
**问答QA**: 使用在SQUAD2.0上微调过的Albert-Xlarge作为QA模型，找到答案a'
**对比评价**:  如果a与a'完全相同，直接输出1；如果QA过程没有找到答案，直接输出0；其他情况下将a和a‘送入NLI模型，这里使用在SNLI数据集上微调过的Roberta, 判断文本蕴含情况。如果QG阶段没有找到合适的问题，那么将r和k送入NLI模型判断，最后取平均计算Q2分数
**实验结果**:<br>
实验在三个数据集上进行，作者人工标注了每个response的事实一致性<br>
![](img/2022-03-37.png)<br>
Q2与其他Baseline模型的p-r曲线对比，可以看出是Q2确实能更好的区分事实一致性. 同时在其他数据集上的实验结果，以及用其他PLM做QA/QG模型的实验结果表明所提方法在各种情况下都比较有效。

<b> 总结 </b> 这篇文章是对之前文本摘要领域做事实一致性评价最新的QG/QA方法([1],[2]为代表)在对话领域的应用，效果还可以，但是创新性比较普通。作者在分析阶段也说了，对话上下文中的闲聊语句会导致模型生成错误的问题，他们也没针对这一点做改进. 而是留给following work(如[4], [5])<br>

<b>相关工作</b><br>
[1]  [Asking and Answering Questions to Evaluate the Factual Consistency of Summaries](https://aclanthology.org/2020.acl-main.450.pdf)<br>
[2]  [FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization](https://aclanthology.org/2020.acl-main.454.pdf)<br>
[3]  [On Faithfulness and Factuality in Abstractive Summarization](https://aclanthology.org/2020.acl-main.173.pdf)<br>
[4] [Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features](https://aclanthology.org/2021.acl-long.58.pdf)<br>
[5] [Improving Factual Consistency of Abstractive Summarization via Question Answering](https://aclanthology.org/2021.acl-long.536.pdf)<br>

by *Baopu Qiu*<br><br>


<b>Title</b> Detecting Hallucinated Content in Conditional Neural Sequence Generation(2021-acl-findings)<br>
<b>简介</b> 提出一种基于预训练模型的、token级别的 文本生成幻觉现象检测模型；给出了一个自建数据集；token级别的幻觉标签可以应用于优化low-resource MT中self-training和noisy bitext mining的训练目标，也可以应用到word-level QE中<br>
<b>背景</b> 幻觉现象在各种文本生成任务里都有出现，先前工作指出已有的评价指标BLEU/ROUGE/BERTscore等不太能识别幻觉（不太相信作者说的，这里提到的先前工作中的一篇是上面的Rico-acl2020，里面并没有说），之前也有一些论文用QA/QG（上篇paper note的相关工作）或者NLI[1]检测幻觉，但他们是在句子级别。<br>
<b>方法</b><br>
**幻觉的分类**：<br>
 - 外在幻觉(extrinsic hallucination)，target中的某个span在source中没有对应的，那么就称这个span有外在幻觉<br>
 - 内在幻觉(intrinsic hallucination)，target中的某个span在source有对应的，但是对应错了，比如本来应该翻译成Mike,但是输出却是Tom，那么就称这个span有内在幻觉<br>
按照作者这种分类的话，机器翻译中的所有错误都可以归类为，漏译+幻觉(多译、错译)+翻译腔（风格问题)，作者在实践中不区分以上类型，把幻觉识别处理成token级别序列二分类标注任务。

**构造训练集**: 为了训练模型，需要构造含有（S,G,L_G）的训练集，作者提出了一种自动生成训练数据的方法：对于双语语料（S,T)，随机mask掉T中的一些token，然后送入BART模型自动补全，得到T'，再计算T与T’的编辑距离，以此给出T‘中每个token的幻觉标签

**训练模型**: 将（S,T,T')作为PLM的输入，输出是T'的幻觉标签序列。为什么要加入T？作者没说，反而说了很多“如何避免过于依赖T”的方法，包括使用dropout/将Tparaphrase成D，再生成D’，用（S, T,D')作为模型输入。此外，还添加了MLM 作为loss的一部分（感觉训练的时候用了很多trick）

<b> 实验与分析 </b><br>
实验分别MT和文本摘要两个任务上进行，MT使用XLM-R, 摘要使用Roberta，作为对比作者提了三个基于规则的baseline(基于词对齐、基于词语overlap、基于同义词)，<br>
![](img/2022-03-38.png)<br>
发现基于规则的方法，在摘要生成的幻觉识别中意外的很好（MT中极差），作者猜想是Roberta的输入长度限制了自己所提方法的性能。<br>
**用于词级别QE**：有监督实验中使用文中的L_PRED+L_MLM损失微调XLM-R，发现效果比WMT18第一名还好；无监督实验中，也大大超过了WMT的无监督第一名方法（因为Word-level QE的水平本来就很低）<br>
**用于优化self-training的训练**： 在NMT的Self-training过程中，对teacher model生成的伪平行语料打上幻觉标签，然后student模型训练时不计算这些标签的loss/或者解码时mask掉它们的attention，实验发现这种做法带来提升 <br>
**用于优化低资源MT的训练**：因为低资源语言的数据集非常noisy，所以在训练时可以用跟上面相同的做法，训练时不计算幻觉输出词的loss，实验表明作者的方法确实在不同噪声水平下都提升了模型性能<br>

<b>总结与反思</b><br>
本文是首个将幻觉定义在token-level的工作，所提出的生成训练数据的方法简单直接，效果也还不错，实验也证明了这项任务对于NMT的训练优化，以及QE都可以产生贡献，是挺好的文章。一点译文，在于作者提出的幻觉生成方法，即随机去掉几个词再让BART补全，是否与真实文本生成任务中出现的幻觉现象一致呢？考虑到上面几篇文章中提到的幻觉现象远比这复杂的多，或许这是未来的改进方向。<br>

by *Baopu Qiu*<br><br>
